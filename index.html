<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Michael Luo</title>
  
  <meta name="author" content="Michael Luo">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Michael Luo</name>
              </p>
              <p>I am a 5th year Masters student (2020-2021) in the <a href="https://eecs.berkeley.edu/">UC Berkeley EECS</a> department studying Artificial Intelligence, Systems, and Robotics. I am advised by <a href="https://people.eecs.berkeley.edu/~istoica/">Prof. Ion Stoica</a> and am associated with <a href="https://eecs.berkeley.edu/">Berkeley Artificial Intelligence Research</a> (BAIR) and <a href="https://rise.cs.berkeley.edu/">RISELab</a>. I have also recently done research under <a href="https://goldberg.berkeley.edu/">Ken Goldberg</a> in <a href="http://autolab.berkeley.edu/">AUTOLab</a>.  
              </p>
              <p>
                Before the Masters program, I earned my B.S. from UC Berkeley with a double major in <a href="https://eecs.berkeley.edu/">EECS</a> and <a href="https://haas.berkeley.edu/">Business Administration</a>. Outside of school, I develop and maintain a popular distributed RL library <a href="https://github.com/ray-project/ray/tree/master/rllib">Ray/RLlib</a> with over 14k stars on Github. In my free time, I enjoy a mix of hobbies, which include powerlifting, playing the piano, and competitive programming.
              </p>
              <p style="text-align:center">
                <a href="mailto:michael.luo@berkeley.edu">Email</a> &nbsp/&nbsp
                <a href="data/Michael_Luo_CV.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?hl=en&user=XpO6-kEAAAAJ">Google Scholar</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/michaelzhiluo/">LinkedIn</a> &nbsp/&nbsp
                <a href="https://github.com/michaelzhiluo/">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/michael_luo.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/michael_luo.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                My masters's and undergraduate research focuses on practical problems that RL faces in real life. This includes scaling up data collection with distributed RL, learning safely with safe RL, and adapting quickly to real-life enviornments with meta RL. I am also interested in applying RL to beat existing algorithms and models in niche fields, such as NLP, query optimization for databases, and video streaming.
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/impact.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/1912.00167">
                <papertitle>IMPACT: Importance Weighted Asynchronous Architectures with Clipped Target Networks</papertitle>
              </a>
              <br>
              
              <strong>Michael Luo</strong>,
              Jiahao Yao,
              Richard Liaw,
              Eric Liang,
              Ion Stoica
              <br>
              <em>International Conference on Learning Representations (ICLR)</em>, 2020  
              <br>
              <a href="https://arxiv.org/abs/1912.00167">Arxiv</a> |
              <a href="https://iclr.cc/virtual_2020/poster_BJeGlJStPr.html">Video</a> |
              <a href="https://github.com/ray-project/ray/blob/master/rllib/agents/ppo/appo.py">Code</a>
              <p></p>
              <p> An algorithm for distributed reinforcement learning that tunes the tradeoff between distributed data collection and learning sample efficiency to optimize for training speed by combining the sample efficiency of PPO and the data throughput from IMPALA.
              </p>
            </td>
          </tr> 

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img style="width:105%;max-width:105%" src="images/cognitive.png">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2011.12719">
                <papertitle>Connecting Context-specific Adaptation in Humans to Meta-learning</papertitle>
              </a>
              <br>
              Rachit Dubey*, 
              Erin Grant*, 
              <strong>Michael Luo*</strong>,
              Karthik Narasimhan,
              Thomas L. Griffiths
              <br>
              <em>Under Review at Conference on Artifical Intelligence (AAAI), 2021.</em>
              <br>
              <a href="https://arxiv.org/abs/2011.12719">Arxiv</a> |
              <a href="https://github.com/michaelzhiluo/ray/tree/metaworld">Code</a>
              <br>
              <p></p>
              <p>We introduce a framework for using contextual information about a task to guide the initialization of task-specific models before adaptation to online feedback, which leads to faster adaptation to online feedback than that of zero-shot multitask approaches.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img style="width:105%;max-width:105%" src="images/recovery-rl.png">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://sites.google.com/berkeley.edu/recovery-rl/">
                <papertitle>Recovery RL: Safe Reinforcement Learning with Learned Recovery Zones</papertitle>
              </a>
              <br>
              Brijen Thananjeyan*,
              Ashwin Balakrishna*,
              Suraj Nair,
              <strong>Michael Luo</strong>,
              Krishnan Srinivasan,
              Minho Hwang,
              Joseph E. Gonzalez,
              Julian Ibarz,
              Chelsea Finn,
              Ken Goldberg
              <br>
              <em>NeurIPS Robot Learning Workshop, 2020.</em>
              <br>
              <a href="https://sites.google.com/berkeley.edu/recovery-rl/">Website</a> |
              <a href="https://arxiv.org/abs/2010.15920">Arxiv</a> |
              <a href="">Code</a>
              <br>
              <p></p>
              <p>An algorithm for safe reinforcement learning which utilizes a set of offline data to learn about constraints before policy learning and a pair of policies which separate the often conflicting objectives of task directed exploration and constraint satisfaction to learn contact rich and visuomotor control tasks.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img style="width:105%;max-width:105%" src="images/dataflow.png">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2011.12719">
                <papertitle>Distributed Reinforcement Learning is a Dataflow Problem</papertitle>
              </a>
              <br>
              Eric Liang*,
              Zhanghao Wu*,
              <strong>Michael Luo</strong>,
              Sven Mika, 
              Ion Stoica
              <br>
              <em>Under Review at Machine Learning and Systems (MLSys) 2021.</em>
              <br>
              <a href="https://arxiv.org/abs/2011.12719">Arxiv</a> |
              <a href="https://github.com/ray-project/ray/blob/master/rllib/">Code (In RLlib)</a>
              <br>
              <p></p>
              <p>We propose RLFlow, a hybrid actor-dataflow programming model for distributed RL, that leads to highly composable and performant implementations of RL algorithms, which results to faster training and significant code reductions.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img style="width:105%;max-width:105%" src="images/voi.png">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://openreview.net/forum?id=jP1vTH3inC">
                <papertitle>Discovering Autoregressive Orderings with Variational Inference</papertitle>
              </a>
              <br>
              Xuanlin Li*,
              Brandon Trabucco*,
              <strong>Michael Luo</strong>,
              Dong Huk Park, 
              Yang Gao,
              Sheng Shen,
              Trevor Darrell
              <br>
              <em>Under Review at International Conference on Learning Representations (ICLR) 2021. (Reviews in Top 7% of Submissions)</em>
              <br>
              <a href="https://openreview.net/forum?id=jP1vTH3inC">Paper</a>
              <br>
              <p></p>
              <p>We propose the first domain-independent unsupervised / self-supervised learner that discovers high-quality autoregressive orders through fully-parallelizable end-to-end training without domain-specific tuning.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img style="width:105%;max-width:105%" src="images/lazydagger.png">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://sites.google.com/view/lazydagger/home">
                <papertitle>LazyDAgger: Reducing Context Switching in Interactive Robot Imitation Learning</papertitle>
              </a>
              <br>
              Ryan Hoque,
              Ashwin Balakrishna,
              Brijen Thanajeyan,
              <strong>Michael Luo</strong>,
              Daniel Seita,
              Daniel S. Brown,
              Ken Goldberg
              <br>
              <em>Under Review at International Conference on Robotics and Automation (ICRA) 2021.</em>
              <br>
              <a href="https://sites.google.com/view/lazydagger/home">Website</a> |
              <a href="https://drive.google.com/file/d/1txjQrOjuRQEyu2WOQ1p4PwWiSksG6jBZ/view">Paper</a>
              <br>
              <p></p>
              <p>An algorithm for interactive imitation learning that learns to minimize human context switching through sustained interventions and maintains the same supervisor burden for prior algorithms.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img style="width:105%;max-width:105%" src="images/garden.png">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="http://alphagarden.org/">
                <papertitle>AlphaGarden: Learning Seed Placement and Automation Policies For Polyculture Farming with Companion Plants</papertitle>
              </a>
              <br>
              Ryan Hoque,
              Ashwin Balakrishna,
              Brijen Thanajeyan,
              <strong>Michael Luo</strong>,
              Daniel Seita,
              Daniel S. Brown,
              Ken Goldberg
              <br>
              <em>Under Review at International Conference on Robotics and Automation (ICRA) 2021.</em>
              <br>
              <a href="http://alphagarden.org/">Website</a> |
              <a href="https://drive.google.com/file/d/1aDfWkznPHM5c9eedJhdXy3DB1F9r8pmq/view?usp=sharing">Paper</a> |
              <a href="https://github.com/BerkeleyAutomation/AlphaGarden">Code</a> 
              <br>
              <p></p>
              <p>We investigate different seed placement and pruning algoritms in a polyculture garden simulator to jointly maximize diveristy and coverage of various plants types.</p>
            </td>
          </tr>

        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Teaching</heading>
            </td>
          </tr>
        </tbody></table>

        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img style="width:105%;max-width:105%" src="images/berkeley_eecs.png">
            </td>
            <td width="75%" valign="center">
              <p>
                <b>CS 189: Introduction to Machine Learning</b> <br>
                Teaching Assistant: <a href="https://inst.eecs.berkeley.edu/~cs189/fa19/">Fall 2019</a>
              </p>
              <p>
                <b>CS 162: Operating Systems and System Programming</b> <br>
                Reader: <a href="https://inst.eecs.berkeley.edu/~cs162/fa18/">Fall 2018</a>
              </p>
            </td>
          </tr>
        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Work Experience</heading>
            </td>
          </tr>
        </tbody></table>

        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle"><img style="width:105%;max-width:105%" src="images/anyscale.png"></td>
            <td width="75%" valign="center">
            <a href="https://www.anyscale.com/">Anyscale</a>
            <br>
            Software Development Intern
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle"><img style="width:105%;max-width:105%" src="images/amazon.jpg"></td>
            <td width="75%" valign="center">
            <a href="https://www.amazon.com/">Amazon</a>
            <br>
            Software Development Intern
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle"><img style="width:105%;max-width:105%" src="images/meraki.png"></td>
            <td width="75%" valign="center">
            <a href="https://meraki.cisco.com/">Cisco Meraki</a>
            <br>
            Computer Vision Intern
            </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                <a href="https://github.com/jonbarron/jonbarron_website">Website template from Jon Barron</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
